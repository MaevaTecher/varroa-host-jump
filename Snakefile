## Population genetic analysis of Varroa mites infected original and new bee hosts
from scripts.split_fasta_regions import split_fasta
from snakemake.utils import R
import getpass

localrules: getHaps, all

### Set path for input files folders
outDir = "/work/MikheyevU/Maeva/varroa-host-jump/data"
refDir = "/work/MikheyevU/Maeva/varroa-host-jump/ref2020" 
SCRATCH  = "/work/scratch/" + getpass.getuser()
krakenDB = "/work/MikheyevU/kraken_db"

# path for software
fastsimcoal2 = "/work/MikheyevU/Maeva/demography/fsc26_linux64/fsc26"
easySFS = "/apps/unit/MikheyevU/Maeva/easySFS/easySFS.py"

### ------------------------------------------------------------
###  REFERENCE GENOMES HONEY BEE HOSTS AND VARROA MITES
### ------------------------------------------------------------

# to do before: bowtie2-build Amel_HAv3.fasta,ACSNU2.0.fasta hostbee
hostBeeBowtieIndex = refDir + "/bees/hostbee"

# to do before: bowtie2-build NC_001566.fasta,NC_014295.fasta hostbeemtDNA
hostBeeMtBowtieIndex = refDir + "/bees/mtDNA/hostbeemito"

##Varroa destructor genome with chromosome starting by NW_019211454.1 
# to do before: bowtie2-build vdes_3.0.fasta vdes_3
vdRef = refDir + "/destructor/vdes_3_refseq.fasta"
vdRefMASK = refDir + "/destructor/maskedregions_vdes_3.bed"
varroaBowtieIndex = refDir + "/destructor/vdes_3_refseq"

# to do before: bowtie2-build AJ493124.fasta vdnavajas
vdmtDNA = refDir + "/destructor/mtDNA/AJ493124.fasta"
vdmtDNABowtieIndex = refDir + "/destructor/mtDNA/vdnavajas"

CHROMOSOMES = ["NW_019211454.1", "NW_019211455.1", "NW_019211456.1", "NW_019211457.1", "NW_019211458.1", "NW_019211459.1", "NW_019211460.1"] 

### ------------------------------------------------------------
### PARAMETERS FOR GENOMIC ANALYSES
### ------------------------------------------------------------

# Input fastq.gz files generated by whole genome sequencing from 44 individuals
SAMPLES, = glob_wildcards(outDir + "/reads/{sample}-R1_001.fastq.gz")

# Splitting Varroa destructor reference genome to call variants in parallel
SPLITS = range(200)
REGIONS = split_fasta(vdRef, len(SPLITS))  # dictionary with regions to be called, with keys in SPLITS
Q = (20, 40) # 99 and 99.99% mapping accuracy
for region in REGIONS:
	for idx,i in enumerate(REGIONS[region]):
		REGIONS[region][idx] = " -r " + str(i)

SPLITSMT = range(10)
REGIONSMT = split_fasta(vdmtDNA, len(SPLITSMT))  # dictionary with regions to be called, with keys in SPLITS
Q = (20, 40) # 99 and 99.99% mapping accuracy
for regionmt in REGIONSMT:
        for idx,i in enumerate(REGIONSMT[regionmt]):
                REGIONSMT[regionmt][idx] = " -r " + str(i)

# Numbers of clusters for NGSadmix analyses
KCLUSTERS = range (1,21)

# Number of runs
RUNS = ["run1", "run2", "run3", "run4", "run5"]

# Parameters for demographic inferences
MODELS = ["M1_div", "M2_IM", "M3_IMtoAM", "M4_IMtoAC", "M5_botdiv", "M6_botIM", "M7_botIMtOAM", "M8_botIMtOAC"]

# different species or pop-species for which SFS was computed and will be run independently
TARGETS = ["destructor", "jacobsoni", "png_jacobsoni"]

### ------------------------------------------------------------
### PSEUDO RULE FOR BUILD-TARGET
### ------------------------------------------------------------

# Change target here depending on the output and step you will like to run up to
rule all: 
	input:	expand(outDir + "/meta/hosts/{sample}-{q}.txt", q = 20, sample = SAMPLES),
		expand(outDir + "/sketches/{sample}.fastq.gz", sample = SAMPLES),
		expand(outDir + "/alignments/bowtie2/{sample}.bam", sample = SAMPLES),
		expand(outDir + "/alignments/ngm/{sample}.bam", sample = SAMPLES),
		expand(outDir + "/alignments/ngm_mtDNA/{sample}.bam", sample = SAMPLES),
		#outDir + "/var/ngm_mtDNA/raw_mtDNA.vcf",
		outDir + "/var/ngm/filtered.vcf",
		outDir + "/var/bowtie2/filtered.vcf",
		outDir + "/var/primitives-sortind_maf005.vcf.gz",
		expand(outDir + "/ngsadmix/all43/{run}/all43_{kcluster}.fopt.gz", kcluster = KCLUSTERS, run = RUNS)
		#outDir + "/R/VJP13.txt",
		#outDir + "/R/VJcer.txt",
		#outDir + "/R/VDcer.txt",
		#outDir + "/R/VJmel.txt",
                #outDir + "/R/VDmel.txt",
		#outDir + "/R/VJP13.outflank.rds"
		#outDir + "/R/VDK.outflank.rds"
		#expand(outDir + "/ngsadmix/vdonly/{run}/vd_{kcluster}.fopt.gz", kcluster = KCLUSTERS, run = RUNS),
		#expand(outDir + "/ngsadmix/vjonly/{run}/vj_{kcluster}.fopt.gz", kcluster = KCLUSTERS, run = RUNS)

### ------------------------------------------------------------
### PART 1 CHECK HONEY BEE HOST IDENTITY
### ------------------------------------------------------------

# Mapping samples reads onto A. mellifera and A. cerana reference mtDNA genomes

# We checked every fastq file (2 per individual) using FastQC to generate a full report on reads quality. All the sequences subsequently used all passed this first visual quality screening.

rule checkHost:
# to implement in rule all if you only want to have the file with exact number of reads per host
# expand(outDir + "/meta/hosts/{sample}-{q}.txt", q = Q, sample = SAMPLES)
	input:
		read1 = outDir + "/reads/{sample}-R1_001.fastq.gz",
		read2 = outDir + "/reads/{sample}-R2_001.fastq.gz",
	output:
		temp(outDir + "/meta/hosts/{sample}-{q}.txt")
	threads: 12
	shell:
		"""
		module load bowtie2/2.2.6 samtools/1.3.1
		bowtie2 -p {threads} -x {hostBeeMtBowtieIndex} -1  {input.read1} -2 {input.read2} | samtools view -S -q {wildcards.q}  -F4 - | awk -v mellifera=0 -v cerana=0 -v sample={wildcards.sample} '$3~/^NC_001566.1/ {{mellifera++; next}}  {{cerana++}} END {{if(mellifera>cerana) print sample"\\tmellifera\\t"cerana"\\t"mellifera ; else print sample"\\tcerana\\t"cerana"\\t"mellifera}}' > {output}
		"""

rule combineHost:
	input:
		expand(outDir + "/meta/hosts/{sample}-{{q}}.txt", sample = SAMPLES)
	output:
		outDir + "/meta/hosts/hosts-{q}.txt"
	shell:
		"""
		(echo -ne "id\\thost\\tcerana\\tmellifera\\n"; cat {input}) > {output}
		"""

rule removeHost:
# to implement in rule all if we only want the files depleted from honey bee hosts reads
# expand(outDir + "/sketches/{sample}.fastq.gz", sample = SAMPLES)
	input:
		read1 = outDir + "/reads/{sample}-R1_001.fastq.gz",
		read2 = outDir + "/reads/{sample}-R2_001.fastq.gz",
	threads: 12
	output: temp(outDir + "/sketches/{sample}.fastq.gz")
	shell: 
		"""
		module load bowtie2/2.2.6 samtools/1.3.1
		bowtie2 -p {threads} -x {hostBeeBowtieIndex} -1  {input.read1} -2 {input.read2}  | samtools view -S -f12 | awk '{{print "@"$1"\\n"$10"\\n+\\n"$11}}' | gzip > {output}
		"""

rule mashtree:
# This step is very useful if you want to quickly check for outliers simply using fastq file
	input: expand(outDir + "/sketches/{sample}.fastq.gz", sample = SAMPLES)
        output: tree = outDir + "/sketches/varroa.dnd", matrix = outDir + "/sketches/varroa.phylip"
        threads: 12
        shell: 	"""
		mashtree.pl --genomesize 500000000 --mindepth 2 --tempdir /work/MikheyevU/Maeva/varroahost/scratch --numcpus {threads} --outmatrix {output.matrix} {input} > {output.tree}
		"""


### ------------------------------------------------------------
### PART 2 MAPPING READS ON VARROA GENOME AND VARIANT CALLING
### ------------------------------------------------------------

# Here reads will be mapped using either Bowtie2 or NextGenMap, then test which one is the best
#  on whole nuclear genome

rule bowtie2:
# NB: we do not trim our reads before mapping as Bowtie2 in very sensitive local mode doesn't require reads to align end-to-end.
# reads are sorted and duplicates were removed using samtools. 
# As massive read-pileups can occur at repetitive regions we reduced this effect with VariantBam by subsampling to a max-coverage of 500.
	input:
		read1 = outDir + "/reads/{sample}-R1_001.fastq.gz",
		read2 = outDir + "/reads/{sample}-R2_001.fastq.gz",
	threads: 12
	output: 
		alignment = temp(outDir + "/alignments/bowtie2/{sample}.bam"), 
		index = temp(outDir + "/alignments/bowtie2/{sample}.bam.bai"),
		read1 = outDir + "/reads_unmapped/{sample}.1",
		read2 = outDir + "/reads_unmapped/{sample}.2"

	shell:
		"""
		module load bowtie2/2.2.6 samtools/1.3.1 VariantBam/1.4.3
		bowtie2 -p {threads} --very-sensitive-local --sam-rg ID:{wildcards.sample} --sam-rg LB:Nextera --sam-rg SM:{wildcards.sample} --sam-rg PL:ILLUMINA  --un-conc-gz  {outDir}/reads_unmapped/{wildcards.sample} -x {varroaBowtieIndex} -1 {input.read1} -2 {input.read2} | samtools view -Su - | samtools sort - -m 55G -T {SCRATCH}/bowtie/{wildcards.sample} -o - | samtools rmdup - - | variant - -m 500 -b -o {output.alignment}
		samtools index {output.alignment}
		"""


rule nextgenmap:
	input:
		read1 = outDir + "/reads/{sample}-R1_001.fastq.gz",
		read2 = outDir + "/reads/{sample}-R2_001.fastq.gz",
	threads: 6
	output: 
		alignment = temp(outDir + "/alignments/ngm/{sample}.bam"), 
		index = temp(outDir + "/alignments/ngm/{sample}.bam.bai")
	shell:
		"""
		module load NextGenMap/0.5.0 samtools/1.3.1 VariantBam/1.4.3
		ngm -t {threads} -b  -1 {input.read1} -2 {input.read2} -r {vdRef} --rg-id {wildcards.sample} --rg-sm {wildcards.sample} --rg-pl ILLUMINA --rg-lb {wildcards.sample} | samtools sort - -m 50G -T {SCRATCH}/ngm/{wildcards.sample} -o - | samtools rmdup - - | variant - -m 500 -b -o {output.alignment}
		samtools index {output.alignment}
		"""

# To get the proportion of reads that mapped to the reference fasta
# samtools flagstat FILE.bam | awk -F "[(|%]" 'NR == 3 {print $2}'
# To get the mean Read Depth
# samtools depth -a FILE.bam | awk '{c++;s+=$3}END{print s/c}'
	
rule freeBayes:
# remember that here FreeBayes will be run in 200 batches as we split the genome in 200 regions for fast computation.
	input: 
		expand(outDir + "/alignments/{{aligner}}/{sample}.bam", sample = SAMPLES)
	output: 
		temp(outDir + "/var/{aligner}/split/freebayes.{region}.vcf")
	params: 
		span = lambda wildcards: REGIONS[wildcards.region],
		bams = lambda wildcards, input: os.path.dirname(input[0]) + "/*.bam",
		missing = lambda wildcards, input: len(input) * 0.9
	shell:
		"""
		module load freebayes/1.1.0 vcftools/0.1.12b vcflib/1.0.0-rc1
		for i in {params.bams}; do name=$(basename $i .bam); if [[ $name == VJ* ]] ; then echo $name VJ; else echo $name VD; fi ; done > {outDir}/var/pops.txt
		freebayes --min-alternate-fraction 0.2 --use-best-n-alleles 4 -m 5 -q 5 --populations {outDir}/var/pops.txt -b {params.bams} {params.span}  -f {vdRef} | vcffilter  -f "QUAL > 20 & NS > {params.missing}" > {output}
		"""

rule mergeVCF:
# the previous split vcf files are merged here in one and vcfuniq is used to remove records which have the same position.
	input: 
		expand(outDir + "/var/{{aligner}}/split/freebayes.{region}.vcf", region = REGIONS)
	output:
		temp(outDir + "/var/{aligner}/raw.vcf")
	shell:
		"""
		module load vcflib/1.0.0-rc1
		(grep "^#" {input[0]} ; cat {input} | grep -v "^#" ) | vcfuniq  > {output}
		"""

# Before this next filtering step, be sure to have run the following script written by D. Cook
# https://www.danielecook.com/generate-a-bedfile-of-masked-ranges-a-fasta-file/
# python generate_masked_ranges.py {VdRef} > maskedregions_vdes_3.txt

rule filterVCF:
# see http://ddocent.com//filtering/
# filter DP  + 3*sqrt(DP) https://arxiv.org/pdf/1404.0929.pdf
# We remove all sites with more than two alleles, but also sites included in the masked highly repetitive or low complexity regions of the genome (small atcg letters).
	input:
		rules.mergeVCF.output
	output:
		vcf = outDir + "/var/{aligner}/filtered.vcf"
	shell:
		"""
		module load vcftools/0.1.12b vcflib/1.0.0-rc1 eplot/2.08
		perl  -ne 'print "$1\\n" if /DP=(\d+)/'  {input} > {outDir}/var/{wildcards.aligner}/depth.txt
		sort -n {outDir}/var/{wildcards.aligner}/depth.txt | uniq -c | awk '{{print $2,$1}}' | eplot -d -r [200:2000] 2>/dev/null | tee 
		Nind=$(grep -m1 "^#C" {input}  | cut -f10- |wc -w)
		coverageCutoff=$(awk -v Nind=$Nind '{{sum+=$1}} END {{print "DP < "(sum / NR / Nind + sqrt(sum / NR / Nind) * 3 ) * Nind}}' {outDir}/var/{wildcards.aligner}/depth.txt)
		echo Using coverage cutoff $coverageCutoff
		vcffilter -s -f \"$coverageCutoff\" {input} | vcftools --vcf - --exclude-bed {vdRefMASK} --max-alleles 2 --recode --chr NW_019211454.1 --chr NW_019211455.1 --chr NW_019211456.1 --chr NW_019211457.1 --chr NW_019211458.1 --chr NW_019211459.1 --chr NW_019211460.1 --stdout > {output}
		"""

rule chooseMapper:
# The results are very similar between the two mappers, so we're going with the one that has the greatest number of variants
	input:
		ngm = outDir + "/var/ngm/filtered.vcf", 
		bowtie2 = outDir + "/var/bowtie2/filtered.vcf", 
	output:
		bgzip = outDir + "/var/filtered.vcf.gz",
		primitives = outDir + "/var/primitives.vcf.gz"
	shell:
		"""
		module load samtools/1.3.1 vcflib/1.0.0-rc1
		ngm=$(grep -vc "^#" {input.ngm})
		bowtie2=$(grep -vc "^#" {input.bowtie2})
		echo ngm has $ngm snps vs $bowtie2 for bowtie2 
		if [[ $ngm -gt $bowtie2 ]]; then
			echo choosing ngm
			bgzip -c {input.ngm} > {output.bgzip}
			vcftools --vcf {input.ngm} --recode --remove-indels --stdout | vcfallelicprimitives | bgzip > {output.primitives}
		else
			echo choosing bowtie2
			bgzip -c {input.bowtie2} > {output.bgzip}
			vcftools --vcf {input.bowtie2} --recode --remove-indels --stdout | vcfallelicprimitives  | bgzip > {output.primitives}
		fi
		tabix -p vcf {output.bgzip} && tabix -p vcf {output.primitives}
		"""

# At this point we go with NextGenMap, which produces a bit more variants
# Followed steps described by https://speciationgenomics.github.io/filtering_vcfs
# to check the missingness, site quality, etc for R

#### ------------------------------------------------------------
#### PART 3 TRANSFORM VCF TO PLINK AND LD PRUNING
#### ------------------------------------------------------------

rule vcfsort_maf:
        input:  vcf = outDir + "/var/primitives.vcf.gz",
                list = outDir + "/list/varroa_ind_2020.txt"
        output: outDir + "/var/primitives-sortind_maf005.vcf.gz"
        shell:
                """
                bcftools view --samples-file {input.list} --output-file {output} -Oz --min-af 0.05 {input.vcf}
                tabix -p vcf {output}
                """

#Plink needs chromosome which are numbered 1 to X, so we need to rename here the original ID of Varroa chromosome by these numbers.
rule ready4plink:
        input: outDir + "/var/primitives-sortind_maf005.vcf.gz"
        output: outDir + "/plink/primitives-sortind-ready4plink.vcf"
        shell:
                """
                gunzip -c {input} > {output}
                sed -i 's/NW_019211454.1/1/g' {output}
                sed -i 's/NW_019211455.1/2/g' {output}
                sed -i 's/NW_019211456.1/3/g' {output}
                sed -i 's/NW_019211457.1/4/g' {output}
                sed -i 's/NW_019211458.1/5/g' {output}
                sed -i 's/NW_019211459.1/6/g' {output}
                sed -i 's/NW_019211460.1/7/g' {output}
                """

# we use plink to prune our SNP and minimize the linkage beteween our SNP filtered
rule vcf2plink:
        input: outDir + "/var/primitives-sortind-ready4plink.vcf"
        output: outDir + "/LDprune/43indplink"
        shell:
                """
                vcftools --vcf {input} --out {output} --plink --chr 1 --chr 2 --chr 3 --chr 4 --chr 5 --chr 6 --chr 7
                """

rule ldprune:
        input: outDir + "/LDprune/43indplink"
        output: outDir + "/LDprune/43ind-ldpruned"
        shell:
                """
                module load plink/1.90b3.36
                plink --file {input} --indep-pairwise 50 5 0.5 --out {output}
                """

# Now we need to revert the process and transform the list of SNP to keep the same name as in the original VCF file
rule SNPpruning:
        input:  list = outDir + "/LDprune/43ind-ldpruned.prune.in",
                vcf = outDir + "/var/primitives-sortind_maf005.vcf.gz"
        output: keeping =  outDir + "/LDprune/prune2keep.txt",
                pruned = outDir + "/LDprune/primitives-sortind-pruned"
        shell:
                """
                cp {input.list} {output.keeping}
                sed -i 's/1:/NW_019211454.1\t/g' {output.keeping}
                sed -i 's/2:/NW_019211455.1\t/g' {output.keeping}
                sed -i 's/3:/NW_019211456.1\t/g' {output.keeping}
                sed -i 's/4:/NW_019211457.1\t/g' {output.keeping}
                sed -i 's/5:/NW_019211458.1\t/g' {output.keeping}
                sed -i 's/6:/NW_019211459.1\t/g' {output.keeping}
                sed -i 's/7:/NW_019211460.1\t/g' {output.keeping}
                vcftools --gzvcf {input.vcf} --positions {output.keeping} --recode --recode-INFO-all --out {output.pruned}
                """

#### ------------------------------------------------------------
#### PART 4 MAPPING READS ON VARROA MITOGENOME AND VARIANT CALLING
#### ------------------------------------------------------------

rule mtDNA_ngm:
        input:
                read1 = outDir + "/reads/{sample}-R1_001.fastq.gz",
                read2 = outDir + "/reads/{sample}-R2_001.fastq.gz",
        threads: 12
        output:
                alignment = temp(outDir + "/alignments/ngm_mtDNA/{sample}.bam"),
                index = temp(outDir + "/alignments/ngm_mtDNA/{sample}.bam.bai")
        shell:
                """
                ngm -t {threads} -b  -1 {input.read1} -2 {input.read2} -r {vdmtDNA} --rg-id {wildcards.sample} --rg-sm {wildcards.sample} --rg-pl ILLUMINA --rg-lb {wildcards.sample} | samtools view -Su -F4 -q10 | samtools sort - -m 55G -T {SCRATCH}/ngm_mtDNA/{wildcards.sample} -o - | samtools rmdup - - | variant - -m 500 -b -o {output.alignment}
                samtools index {output.alignment}
                """

rule mtDNA_freeBayes:
        input:
                expand(outDir + "/alignments/ngm_mtDNA/{sample}.bam", sample = SAMPLES)
        output:
                temp(outDir + "/var/ngm_mtDNA/split_mtDNA/freebayes_mtDNA.{regionmt}.vcf")
        params:
                span = lambda wildcards: REGIONSMT[wildcards.regionmt],
                bams = lambda wildcards, input: os.path.dirname(input[0]) + "/*.bam",
                missing = lambda wildcards, input: len(input) * 0.9
        shell:
                """
                 for i in {params.bams}; do name=$(basename $i .bam); if [[ $name == VJ* ]] ; then echo $name VJ; else echo $name VD; fi ; done > {outDir}/var/pops_mtDNA.txt
                freebayes --ploidy 1 --use-best-n-alleles 4 -m 5 -q 5 --populations {outDir}/var/pops_mtDNA.txt -b {params.bams} {params.span} -f {vdmtDNA} | vcffilter -f "QUAL > 20 & NS > {params.missing}" > {output}
                """

rule mtDNA_mergeVCF:
        input:
                expand(outDir + "/var/ngm_mtDNA/split_mtDNA/freebayes_mtDNA.{regionmt}.vcf", regionmt = REGIONSMT)
        output:
                temp(outDir + "/var/ngm_mtDNA/raw_mtDNA.vcf")
        shell:
                """
                (grep "^#" {input[0]} ; cat {input} | grep -v "^#" ) | vcfuniq  > {output}
                """

rule mtDNA_filterVCF:
        input:
                rules.mtDNA_mergeVCF.output
        output:
                vcf = outDir + "/var/ngm_mtDNA/filtered_mtDNA.vcf"
        shell:
               """
                perl  -ne 'print "$1\\n" if /DP=(\d+)/'  {input} > {outDir}/var/ngm_mtDNA/depth_mtDNA.txt
               sort -n {outDir}/var/ngm_mtDNA/depth_mtDNA.txt | uniq -c | awk '{{print $2,$1}}' | eplot -d -r [200:2000] 2>/dev/null | tee
               Nind=$(grep -m1 "^#C" {input}  | cut -f10- |wc -w)
               coverageCutoff=$(awk -v Nind=$Nind '{{sum+=$1}} END {{print "DP < "(sum / NR / Nind + sqrt(sum / NR / Nind) * 3 ) * Nind}}' {outDir}/var/ngm_mtDNA/depth_mtDNA.txt)
               echo Using coverage cutoff $coverageCutoff
               vcffilter -s -f \"$coverageCutoff\" {input} | vcftools --vcf - --max-alleles 2 --recode --stdout > {output}
               """

## Checking with Sanger sequencing if it is better to extract the fasta from raw or filtered vcf mtDN
### I used finally only the rawmtDNA.vcf fasta file and checked manually for problem that could appear with gap == alignment good
## vcf2fasta -f /var/ngm_mtDNA/fasta/{sample}_AJ493124.2:0.fasta -P1 /var/ngm_mtDNA/raw_mtDNA.vcf
rule vcf2fasta:
                input: 	outDir + "/var/ngm_mtDNA/raw_mtDNA.vcf"
                output: temp(outDir + "/var/ngm_mtDNA/fasta/{sample}_raw_wholemtDNA.fasta")
                shell:
                        """
			vcf2fasta -f {vdmtDNA} -P1 {input}
                        """

#The files generated may have problems with phasing so just remove the problematic SNP. 
#Also to rename the file correctly and also avoid the problem with the first line, use the following script:
#for filename in $(ls -1 *.fasta); do base=$(basename ${filename} "_AJ493124.2:0.fasta"); echo "$base"; sed -i -e "1d" $filename; awk 'BEGIN{print ">'$base'_rawmtDNA"}1' $filename > "$base".fasta; done

 
##### ------------------------------------------------------------
##### PART 5 ANALYSES FST, POPULATION DIFFERENTIATION & STRUCTURE
##### ------------------------------------------------------------

## DO the same for non pruned and pruned dataset
#rule pcagenome:
#        input:  variant = outDir + "/LDprune/primitives-sortind-pruned.recode.vcf",
#                listpop = outDir + "/list/varroaR43i_6POP.txt"
#        output: outDir + "/R/pca-all43-ldpruned.pdf"
#        shell:
#                """
#                module load R/3.4.2
#                Rscript --vanilla /work/MikheyevU/Maeva/varroa-jump/scripts/PCA_vcfR.R {input.variant} {input.listpop} {output}
#                """

# Alternatively we can also use directly plink, which will be making eigen files for R

rule pcaplink:
        input: outDir + "/LDprune/primitives-sortind-pruned.recode.vcf"
        output: outDir + "/LDprune/primitives-sortind-prunedplink"
        shell:
                """
                module load plink/1.90b3.36
                plink --vcf {input} --pca --out {output}
                """


rule VCF012:
# convert vcf to R data frame with meta-data 
# remove maf 0.1, since we don't have much power for them
# we also keep only sites even if the Minor Allele Count is 1
        input: 
                vcf = outDir + "/LDprune/primitives-sortind-pruned.recode.vcf",
                ref = refDir + "/varroa2020.txt"
        output: outDir + "/R/{species}.txt"
        shell: 
                """
                module load vcftools/0.1.12b
                vcftools --gzvcf {input.vcf} --012 --mac 1 --keep <(awk -v species={wildcards.species} '$3==toupper(species) {{print $1 }}' {input.ref}) --maf 0.1 --out data/R/{wildcards.species}
               # transpose loci to columns
                (echo -ne "id\\thost\\tspecies\\t"; cat data/R/{wildcards.species}.012.pos | tr "\\t" "_" | tr "\\n" "\\t" | sed 's/\\t$//'; echo) > {output}
               # take fedHost and species columns and append them to the genotype data file
                paste <(awk 'NR==FNR {{a[$1]=$1"\\t"$5"\\t"$3; next}} $1 in a {{print a[$1]}}' {input.ref} data/R/{wildcards.species}.012.indv) <(cat data/R/{wildcards.species}.012  | cut -f2-) | sed 's/-1/9/g' >> {output}

                """

rule VCF2Dis:
        input: outDir + "/LDprune/primitives-sortind-pruned.recode.vcf"
        output: outDir + "/LDprune/primitives-sortind-pruned.mat"
        shell: "module load VCF2Dis/1.09; VCF2Dis -InPut {input} -OutPut {output}"



rule outflankFst:
# compute FST for outflank analysis
	input:
		outDir + "/R/{species}.txt"
	output: 
		outDir + "/R/{species}.outflank.rds"
	shell:
		"""
		module load R/3.4.2
		Rscript --vanilla /work/MikheyevU/Maeva/varroa-jump/scripts/outflank.R {input} {output}
		"""


rule popstats:
	input:
		vcf = outDir + "/LDprune/primitives-sortind-pruned.recode.vcf",
		ref = refDir + "/varroa.txt"
	output:
		dFst = outDir + "/R/dFst.txt",
		jFst = outDir + "/R/jFst.txt",
		dcStats = outDir + "/R/dcStats.txt",
		dmStats = outDir + "/R/dmStats.txt",
		jcStats = outDir + "/R/jcStats.txt",
		jmStats = outDir + "/R/jmStats.txt",
		dpFst = outDir + "/R/dpFst.txt",
		jpFst = outDir + "/R/jpFst.txt"
	threads: 8
	resources: mem=20, time=12*60
	shell:
		"""
		module load vcflib/1.0.0-rc1 parallel
		dc=$(awk -v host=cerana -v species=VD -v ORS=, '(NR==FNR) {{a[$1]=NR-1; next}} ($2==host) && ($3==species) {{print a[$1]}}'  <(zcat {input.vcf} |grep -m1 "^#C" | cut -f10- | tr "\\t" "\\n") {input.ref} | sed 's/,$//')
		dm=$(awk -v host=mellifera -v species=VD -v ORS=, '(NR==FNR) {{a[$1]=NR-1; next}} ($2==host) && ($3==species) {{print a[$1]}}'  <(zcat data/var/filtered.vcf.gz |grep -m1 "^#C" | cut -f10- | tr "\\t" "\\n") {input.ref} | sed 's/,$//')
		jm=$(awk -v host=mellifera -v species=VJ -v ORS=, '(NR==FNR) {{a[$1]=NR-1; next}} ($2==host) && ($3==species) {{print a[$1]}}'  <(zcat data/var/filtered.vcf.gz |grep -m1 "^#C" | cut -f10- | tr "\\t" "\\n") {input.ref} | sed 's/,$//')
		jc=$(awk -v host=cerana -v species=VJ -v ORS=, '(NR==FNR) {{a[$1]=NR-1; next}} ($2==host) && ($3==species) {{print a[$1]}}'  <(zcat data/var/filtered.vcf.gz |grep -m1 "^#C" | cut -f10- | tr "\\t" "\\n") {input.ref} | sed 's/,$//')
		#compute Fst for both species

		tempfile=$(mktemp)
		echo "wcFst --target $dm --background $dc --file {input} --type GL  > {output.dFst} " >> $tempfile
		echo "wcFst --target $jm --background $jc --file {input} --type GL  > {output.jFst}" >> $tempfile
		echo "pFst --target $dm --background $dc --file {input} --type GL  > {output.dpFst} " >> $tempfile
		echo "pFst --target $jm --background $jc --file {input} --type GL  > {output.jpFst} " >> $tempfile
		#compute descriptive statistics for both species
		echo "popStats --type GL --target $dc --file {input}  > {output.dcStats}" >> $tempfile
		echo "popStats --type GL --target $dm --file {input}  > {output.dmStats}" >> $tempfile
		echo "popStats --type GL --target $jc --file {input}  > {output.jcStats}" >> $tempfile
		echo "popStats --type GL --target $jm --file {input}  > {output.jmStats}" >> $tempfile
		cat $tempfile | xargs -P {threads} -I % sh -c '%'
		rm $tempfile
		"""

# # estimate SNP effects
# rule snpEff:
# 	input: rules.consensusFilter.output
# 	output: "../data/popgen/var/snpEff.txt"
# 	shell: "java -Xmx7g -jar /apps/unit/MikheyevU/sasha/snpEff4/snpEff.jar -no-utr -no-upstream -no-intron -no-intergenic -no-downstream pmuc {input} >  {output}"
# 	""" python parse_silentReplacement.py ../ref/csd.fa temp.txt > {output} && rm temp.txt """

# rule getCDS:
# 	input: GFF, REF
# 	output: "../ref/cds.fa"
# 	shell: "gffread {input[0]} -g {input[1]} -x {output}"

# rule filterLongest:
# 	input: rules.getCDS.output
# 	output: "../ref/longest.fa"
# 	shell: "python filter_longest.py {input} > {output}"

# # determine which SNPs are fixed and which are polymorphic
# # for this we remove the outgroup and compute frequencies
# rule fixedPolymorphic:	
# 	input: rules.consensusFilter.output
# 	output: "../data/popgen/var/snps.csv"
# 	shell: """module load zlib; vcftools --vcf {input} --remove-indv Pflavoviridis --freq; \
#     awk -v OFS="," ' NR>1 {{split($5,a,":"); if((a[2]=="1") || (a[2]=="0")) state="F"; else state="P"; print $1,$2,state}}' out.frq > {output} """

# # exports silent and replacement sites from snpEff
# rule parseSilentReplacement:
# 	input: rules.filterLongest.output, rules.snpEff.output
# 	output: "../data/popgen/var/annotation.csv"
# 	shell: ". ~/python2/bin/activate ; python parse_silentReplacement.py {input} > {output}"

# # calculate how many synonymous vs_non-synonymous changes are possible
# rule silentReplacement:
# 	input: rules.filterLongest.output
# 	output: "../data/popgen/var/silentReplacement.csv"
# 	shell: ". ~/python2/bin/activate; python silent_replacement.py {input} > {output}"

# rule snipre:
# 	input: rules.silentReplacement.output, rules.fixedPolymorphic.output, rules.parseSilentReplacement.output
# 	output: "../out/bayesian_results.csv"
				

###### ------------------------------------------------------------
###### PART 5 ADMIXTURE WITH NGSADMIX
###### ------------------------------------------------------------

# transform in BEAGLE format for NGSadmix
rule vcf2GL:
        input:	outDir + "/LDprune/primitives-sortind-pruned.recode.vcf"
        output:	temp(outDir + "/ngsadmix/all43/{chromosome}.BEAGLE.GL")
        shell:
                """
		vcftools --vcf {input} --chr {wildcards.chromosome} --out /work/MikheyevU/Maeva/varroa-jump/data/ngsadmix/all43/{wildcards.chromosome} --max-missing 1 --BEAGLE-GL
                """

rule mergeGL:
        input: expand(outDir + "/ngsadmix/all43/{chromosome}.BEAGLE.GL", chromosome = CHROMOSOMES)
        output: outDir + "/ngsadmix/all43/sevenchr.BEAGLE.GL"
        shell:
                """
                (head -1 {input[0]}; for i in {input}; do cat $i | sed 1d; done) > {output}
                """

## All individuals NGSadmix
rule NGSadmix:
        input: outDir + "/ngsadmix/all43/sevenchr.BEAGLE.GL"
        threads: 12
        output: temp(outDir + "/ngsadmix/all43/{run}/all43_{kcluster}.fopt.gz")
        shell:
                """
                NGSadmix -P {threads} -likes {input} -K {wildcards.kcluster} -outfiles /work/MikheyevU/Maeva/varroa-jump/data/ngsadmix/all43/{wildcards.run}/all43_{wildcards.kcluster} -minMaf 0.1
                """

## Just for Varroa destructor samples
rule vdonly_vcf2GL:
        input:  outDir + "/LDprune/primitives-sort-pruned.recode.vcf"
        output: temp(outDir + "/ngsadmix/vdonly/{chromosome}.BEAGLE.GL")
        shell:
                """
                vcftools --vcf {input} --chr {wildcards.chromosome} --keep /work/MikheyevU/Maeva/varroa-jump/data/list/vdonly.txt --out /work/MikheyevU/Maeva/varroa-jump/data/ngsadmix/vdonly/{wildcards.chromosome} --max-missing 1 --BEAGLE-GL
                """

rule vdonly_mergeGL:
        input: expand(outDir + "/ngsadmix/vdonly/{chromosome}.BEAGLE.GL", chromosome = CHROMOSOMES)
        output: outDir + "/ngsadmix/vdonly/vdonly.BEAGLE.GL"
        shell:
                """
                (head -1 {input[0]}; for i in {input}; do cat $i | sed 1d; done) > {output}
                """

rule vdonly_admix:
        input: outDir + "/ngsadmix/vdonly/vdonly.BEAGLE.GL"
        threads: 12
        output: temp(outDir + "/ngsadmix/vdonly/{run}/vd_{kcluster}.fopt.gz")
        shell:
                """
                NGSadmix -P {threads} -likes {input} -K {wildcards.kcluster} -outfiles /work/MikheyevU/Maeva/varroa-jump/data/ngsadmix/vdonly/{wildcards.run}/vd_{wildcards.kcluster} -minMaf 0.1
                """

## just for Varroa jacobsoni samples
rule vjonly_vcf2GL:
        input:  outDir + "/LDprune/primitives-sort-pruned.recode.vcf"
        output: temp(outDir + "/ngsadmix/vjonly/{chromosome}.BEAGLE.GL")
        shell:
                """
                vcftools --gzvcf {input} --chr {wildcards.chromosome} --keep /work/MikheyevU/Maeva/varroa-jump/data/list/vjonly.txt --out /work/MikheyevU/Maeva/varroa-jump/data/ngsadmix/vjonly/{wildcards.chromosome} --max-missing 1 --BEAGLE-GL
                """

rule vjonly_mergeGL:
        input: expand(outDir + "/ngsadmix/vjonly/{chromosome}.BEAGLE.GL", chromosome = CHROMOSOMES)
        output: outDir + "/ngsadmix/vjonly/vjonly.BEAGLE.GL"
        shell:
                """
                (head -1 {input[0]}; for i in {input}; do cat $i | sed 1d; done) > {output}
                """

rule vjonly_admix:
        input: outDir + "/ngsadmix/vjonly/vjonly.BEAGLE.GL"
        threads: 12
        output: temp(outDir + "/ngsadmix/vjonly/{run}/vj_{kcluster}.fopt.gz")
        shell:
                """
                NGSadmix -P {threads} -likes {input} -K {wildcards.kcluster} -outfiles /work/MikheyevU/Maeva/varroa-jump/data/ngsadmix/vjonly/{wildcards.run}/vj_{wildcards.kcluster} -minMaf 0.1
                """

####### ------------------------------------------------------------
####### PART 6 DEMOGRAPHIC INFERENCES INPUT
####### ------------------------------------------------------------

## We use here the python script easySFS obtained from https://github.com/isaacovercast/easySFS
# Before running the following rule, use the --preview mode to choose the projection number to balance # of segregating sites against # of samples to avoid downsampling too far.
# here we decided to keep all samples for VDAC so 

#rule vcf2SFS:
#        input:  variant = outDir + "/LDprune/primitives-sort-pruned.recode.vcf",
#		poplist = outDir + "/list/popSFS_vd18i2pop.txt"
#        output: temp(outDir + "/demography/inputMAF")
#        shell:
#                """
#                module load python/2.7.10
#		export PYTHONPATH=/apps/unit/MikheyevU/Maeva/easySFS/lib/python2.7/site-packages
#		
#		easySFS -i {input.variant} -p {input.poplist} -a -o {output} --proj=18,18
#		"""

## prepare the files .tpl and .est for each model tested here using fastsimcoal2, and copy the jointMAF from conversion using easySFS in each model folder
## I noticed that easySFS convert the file for fastsimcoal but must be rename to fit the requirement from 

rule files4fsc:
	input:	tplin = outDir + "/demography/{target}/{model}/{model}.tpl",
		estin = outDir + "/demography/{target}/{model}/{model}.est",
		mafin = outDir + "/demography/{target}/{model}/{model}_jointMAFpop1_0.obs"
	output:	tplout = outDir + "/demography/{target}/{model}/{model}_{iteration}.tpl",
		estout = outDir + "/demography/{target}/{model}/{model}_{iteration}.est",
                mafout = outDir + "/demography/{target}/{model}/{model}_{iteration}_jointMAFpop1_0.obs"
	shell:
		"""
		cp {input.tplin} {output.tplout}
		cp {input.estin} {output.estout}
		cp {input.mafin} {output.mafout}
		"""

#### ERROR IN THE RUN NEED TO FIX IT
#rule runfastsimcoal:
#	input:	folder = temp(outDir + "/demography/{target}/{model}"),
#		tpl = outDir + "/demography/{target}/{model}/{model}_{iteration}.tpl",
#		est = outDir + "/demography/{target}/{model}/{model}_{iteration}.est"
#	output:	temp(outDir + "/demography/{target}/{model}/{model}_{iteration}/{model}_{iteration}.bestlhoods")
#       shell:
#                """
#		cd {input.folder}
#		/work/MikheyevU/Maeva/demography/fsc26_linux64/fsc26 --tplfile {input.tpl} --estfile {input.est} -m --numsims 10000 --maxlhood 0.001 --minnumloops 20 --numloops 50 -c 10
#		
#		"""

#rule getbestboot:
#	input:	title = outDir + "/demography/{target}/{model}/{model}_{iteration}/{model}_1.bestlhoods",
#		likelihood = temp(outDir + "/demography/{target}/{model}/{model}_{iteration}/{model}_{iteration}.bestlhoods")
#	output:	temp(outDir + "/demography/{target}/{model}/{model}_bestmodel.txt")
#	shell:	
#		""
#		cat {input.title} | sed -n 1p >> {output}
#		cat {input} | sed -n 2p >> {output}
#		"""

